{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f20189f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>img</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>53692</td>\n",
       "      <td>img/53692.png</td>\n",
       "      <td>1</td>\n",
       "      <td>wait, mohammad come back!! i didn't mean to la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>78469</td>\n",
       "      <td>img/78469.png</td>\n",
       "      <td>1</td>\n",
       "      <td>when each letter is a mental disorder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>93512</td>\n",
       "      <td>img/93512.png</td>\n",
       "      <td>1</td>\n",
       "      <td>once you go black so does your eye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10829</td>\n",
       "      <td>img/10829.png</td>\n",
       "      <td>1</td>\n",
       "      <td>its ok, let them in only a few will kill</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>49536</td>\n",
       "      <td>img/49536.png</td>\n",
       "      <td>0</td>\n",
       "      <td>hillary clinton caught telling the truth! apri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>995</td>\n",
       "      <td>46283</td>\n",
       "      <td>img/46283.png</td>\n",
       "      <td>0</td>\n",
       "      <td>when you're dedicated to working out everyday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>996</td>\n",
       "      <td>46359</td>\n",
       "      <td>img/46359.png</td>\n",
       "      <td>0</td>\n",
       "      <td>then something amazing started to happen! last...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>997</td>\n",
       "      <td>58061</td>\n",
       "      <td>img/58061.png</td>\n",
       "      <td>1</td>\n",
       "      <td>i now have two identities transgender and tran...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>998</td>\n",
       "      <td>36152</td>\n",
       "      <td>img/36152.png</td>\n",
       "      <td>0</td>\n",
       "      <td>when your eating out your grandma and you tast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>999</td>\n",
       "      <td>90378</td>\n",
       "      <td>img/90378.png</td>\n",
       "      <td>0</td>\n",
       "      <td>every place has their meet up spot after school</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0     id            img  label  \\\n",
       "0             0  53692  img/53692.png      1   \n",
       "1             1  78469  img/78469.png      1   \n",
       "2             2  93512  img/93512.png      1   \n",
       "3             3  10829  img/10829.png      1   \n",
       "4             4  49536  img/49536.png      0   \n",
       "..          ...    ...            ...    ...   \n",
       "995         995  46283  img/46283.png      0   \n",
       "996         996  46359  img/46359.png      0   \n",
       "997         997  58061  img/58061.png      1   \n",
       "998         998  36152  img/36152.png      0   \n",
       "999         999  90378  img/90378.png      0   \n",
       "\n",
       "                                                  text  \n",
       "0    wait, mohammad come back!! i didn't mean to la...  \n",
       "1                when each letter is a mental disorder  \n",
       "2                   once you go black so does your eye  \n",
       "3             its ok, let them in only a few will kill  \n",
       "4    hillary clinton caught telling the truth! apri...  \n",
       "..                                                 ...  \n",
       "995      when you're dedicated to working out everyday  \n",
       "996  then something amazing started to happen! last...  \n",
       "997  i now have two identities transgender and tran...  \n",
       "998  when your eating out your grandma and you tast...  \n",
       "999    every place has their meet up spot after school  \n",
       "\n",
       "[1000 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.read_csv(\"data/training_sample.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358f4737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (model): CLIPModel(\n",
       "    (text_model): CLIPTextTransformer(\n",
       "      (embeddings): CLIPTextEmbeddings(\n",
       "        (token_embedding): Embedding(49408, 512)\n",
       "        (position_embedding): Embedding(77, 512)\n",
       "      )\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "              (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "              (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (vision_model): CLIPVisionTransformer(\n",
       "      (embeddings): CLIPVisionEmbeddings(\n",
       "        (patch_embedding): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n",
       "        (position_embedding): Embedding(50, 768)\n",
       "      )\n",
       "      (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (encoder): CLIPEncoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-11): 12 x CLIPEncoderLayer(\n",
       "            (self_attn): CLIPAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (mlp): CLIPMLP(\n",
       "              (activation_fn): QuickGELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            )\n",
       "            (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (visual_projection): Linear(in_features=768, out_features=512, bias=False)\n",
       "    (text_projection): Linear(in_features=512, out_features=512, bias=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Linear(in_features=1024, out_features=500, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=500, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.config import Config\n",
    "from models.classifiers import BinaryClassifier\n",
    "import torch\n",
    "\n",
    "config=Config()\n",
    "model_test=BinaryClassifier(config)\n",
    "\n",
    "model_test.load_state_dict(torch.load(\"models/checkpoints/bimodal_classifier.pth\"))\n",
    "model_test.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b341cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'attention_mask', 'pixel_values', 'labels'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.dataloader import MultimodalDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from src.config import BimodalConfig\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "bimodal_config=BimodalConfig()\n",
    "model=CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer=processor.tokenizer\n",
    "test_df=pd.read_json(\"data/original_test.jsonl\", lines=True)\n",
    "\n",
    "test_dataset=MultimodalDataset(test_df, processor, bimodal_config.mode)\n",
    "test_loader=DataLoader(test_dataset, batch_size=bimodal_config.batch_size,shuffle=True,num_workers=0)\n",
    "\n",
    "first_batch=next(iter(test_loader))\n",
    "\n",
    "datapoint = {\n",
    "    \"input_ids\": first_batch[\"input_ids\"][0],           # shape: (seq_len,)\n",
    "    \"attention_mask\": first_batch[\"attention_mask\"][0], # shape: (seq_len,)\n",
    "    \"pixel_values\": first_batch[\"pixel_values\"][0],     # shape: (C, H, W)\n",
    "    \"labels\": first_batch[\"labels\"][0]                  # scalar\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc03b178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to outputs/test_attention_heads.png\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "# Dummy attention matrix (e.g. 8x8)\n",
    "attention_matrix = np.random.rand(8, 8)\n",
    "\n",
    "# Dummy tokens\n",
    "tokens = [f\"tok_{i}\" for i in range(8)]\n",
    "\n",
    "# Create a grid of subplots for 8 attention heads\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))  # 2 rows x 4 cols\n",
    "\n",
    "for i in range(8):\n",
    "    ax = axes[i // 4, i % 4]  # Position in subplot grid\n",
    "\n",
    "    sns.heatmap(attention_matrix, xticklabels=tokens, yticklabels=tokens,\n",
    "                cmap=\"viridis\", cbar=False, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Head {i}\", fontsize=10)\n",
    "    ax.tick_params(axis='x', rotation=90, labelsize=6)\n",
    "    ax.tick_params(axis='y', labelsize=6)\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "name=\"test\"\n",
    "\n",
    "# Save the figure\n",
    "plt.savefig(os.path.join(\"interpretability/plots\", name), bbox_inches='tight', dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(\"Saved to outputs/test_attention_heads.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
